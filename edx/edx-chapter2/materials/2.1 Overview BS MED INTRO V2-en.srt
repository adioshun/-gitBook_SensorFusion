0
00:00:00,000 --> 00:00:00,500


1
00:00:00,500 --> 00:00:03,120
Welcome to this section, where we will give an introduction

2
00:00:03,120 --> 00:00:05,190
to Bayesian statistics.

3
00:00:05,190 --> 00:00:08,010
Besides being one of my favorite subjects,

4
00:00:08,010 --> 00:00:10,200
the theory of Bayesian statistics

5
00:00:10,200 --> 00:00:12,900
also underline all nonlinear filtering

6
00:00:12,900 --> 00:00:17,380
and sensor fusion methods that we will present in this course.

7
00:00:17,380 --> 00:00:19,620
Although you can easily spend years studying

8
00:00:19,620 --> 00:00:23,190
all the facets of it, our aim for this section

9
00:00:23,190 --> 00:00:25,320
is to give you a basic understanding

10
00:00:25,320 --> 00:00:28,590
of the concepts which Bayesian statistics is built upon.

11
00:00:28,590 --> 00:00:30,690
In this first lecture, we will focus

12
00:00:30,690 --> 00:00:33,600
on giving you more of an overview of the subject,

13
00:00:33,600 --> 00:00:36,360
without going into too much details.

14
00:00:36,360 --> 00:00:39,180
So what is Bayesian statistics?

15
00:00:39,180 --> 00:00:42,690
Well, it's a statistical inference framework,

16
00:00:42,690 --> 00:00:45,840
which means that we can use it to make decisions or draw

17
00:00:45,840 --> 00:00:49,470
conclusions regarding some parameters of interest using

18
00:00:49,470 --> 00:00:53,220
data where there is typically some inherent uncertainty

19
00:00:53,220 --> 00:00:54,520
in the data.

20
00:00:54,520 --> 00:00:58,530
A typical example is that the data is imperfect and noisy

21
00:00:58,530 --> 00:01:01,920
and can thus not give us exact information regarding

22
00:01:01,920 --> 00:01:03,840
what we are interested in.

23
00:01:03,840 --> 00:01:06,600
Instead, we want to use Bayesian statistics to try

24
00:01:06,600 --> 00:01:09,630
to extract as much good information as possible

25
00:01:09,630 --> 00:01:12,360
from our noisy observations.

26
00:01:12,360 --> 00:01:14,220
In many ways, Bayesian statistics

27
00:01:14,220 --> 00:01:16,110
is a very flexible framework, which

28
00:01:16,110 --> 00:01:19,360
can be used for many different types of decisions.

29
00:01:19,360 --> 00:01:22,110
For example, the same basic theory and methods

30
00:01:22,110 --> 00:01:26,340
can be used for estimation, classification, detection,

31
00:01:26,340 --> 00:01:28,840
and model selection, and so forth.

32
00:01:28,840 --> 00:01:31,560
The key characteristic being that, whatever problem that

33
00:01:31,560 --> 00:01:34,080
we are facing, the unknown quantities

34
00:01:34,080 --> 00:01:36,030
are described as random.

35
00:01:36,030 --> 00:01:39,090
This could, for example, be the type of disease of a patient,

36
00:01:39,090 --> 00:01:42,130
the transmitted message in a communication system,

37
00:01:42,130 --> 00:01:44,640
the temperature in the cylinder in an engine,

38
00:01:44,640 --> 00:01:48,750
or the intention of a driver in a traffic situation.

39
00:01:48,750 --> 00:01:50,760
To make things a bit more concrete,

40
00:01:50,760 --> 00:01:53,040
let's look at using Bayesian statistics

41
00:01:53,040 --> 00:01:54,690
in a medical application.

42
00:01:54,690 --> 00:01:56,310
Let's say that we want to analyze

43
00:01:56,310 --> 00:01:58,140
the disease of a patient.

44
00:01:58,140 --> 00:02:00,950
In this case, the quantity of interest

45
00:02:00,950 --> 00:02:03,480
that we want to apply Bayesian statistics to try

46
00:02:03,480 --> 00:02:07,630
to make a decision on is the disease of the patient.

47
00:02:07,630 --> 00:02:11,730
Let's denote the variable that describes the disease as theta.

48
00:02:11,730 --> 00:02:13,680
To help us make decisions regarding

49
00:02:13,680 --> 00:02:16,860
the disease of the patient, we make some observations.

50
00:02:16,860 --> 00:02:19,770
And we make these observations by collecting and analyzing

51
00:02:19,770 --> 00:02:22,560
blood samples, taking the temperature of the patient,

52
00:02:22,560 --> 00:02:24,600
collecting comments by the patients

53
00:02:24,600 --> 00:02:26,940
on experienced symptoms and so on.

54
00:02:26,940 --> 00:02:29,520
In many cases, no one of these alone

55
00:02:29,520 --> 00:02:31,830
will give us the conclusive answer

56
00:02:31,830 --> 00:02:33,870
to the disease of the patient.

57
00:02:33,870 --> 00:02:36,330
But if we weigh them together, the picture

58
00:02:36,330 --> 00:02:38,610
usually becomes a bit clearer.

59
00:02:38,610 --> 00:02:41,760
And we can use Bayesian statistics to do this.

60
00:02:41,760 --> 00:02:46,440
An important aspect of Bayesian statistics is that theta--

61
00:02:46,440 --> 00:02:48,930
the disease of the patient, in this case--

62
00:02:48,930 --> 00:02:50,760
is described as random.

63
00:02:50,760 --> 00:02:52,710
So it's a random variable.

64
00:02:52,710 --> 00:02:56,160
By doing this, we can calculate the probability distribution

65
00:02:56,160 --> 00:02:59,010
of the disease of the patient based on the observations

66
00:02:59,010 --> 00:03:00,840
that we have made.

67
00:03:00,840 --> 00:03:02,550
For example, we can make statements

68
00:03:02,550 --> 00:03:05,850
like, based on our observations, the patient has

69
00:03:05,850 --> 00:03:09,530
disease x with 97% probability.

70
00:03:09,530 --> 00:03:11,430
A possible concern that one can have

71
00:03:11,430 --> 00:03:14,240
regarding the Bayesian way of viewing things

72
00:03:14,240 --> 00:03:16,740
is whether it's actually valid to say

73
00:03:16,740 --> 00:03:18,900
that the disease is random.

74
00:03:18,900 --> 00:03:22,020
Surely the patient has gotten a disease and not

75
00:03:22,020 --> 00:03:23,770
any random disease.

76
00:03:23,770 --> 00:03:25,980
However, there are many advantages

77
00:03:25,980 --> 00:03:28,230
with viewing the disease as random.

78
00:03:28,230 --> 00:03:30,720
For example, in many situations, we

79
00:03:30,720 --> 00:03:34,230
will not be able to determine with 100% certainty which

80
00:03:34,230 --> 00:03:35,910
disease the patient has.

81
00:03:35,910 --> 00:03:37,830
By then viewing the disease as random,

82
00:03:37,830 --> 00:03:41,190
it allows us to also express our uncertainty

83
00:03:41,190 --> 00:03:43,220
in which type of disease the patient has.

84
00:03:43,220 --> 00:03:47,220
For example, here, perhaps it's 97% probability

85
00:03:47,220 --> 00:03:51,270
that the patient has the flu, but with 3% probability

86
00:03:51,270 --> 00:03:53,100
it's just a common cold.

87
00:03:53,100 --> 00:03:55,320
When deciding on appropriate action,

88
00:03:55,320 --> 00:03:59,560
it's surely beneficial to know with which certainty

89
00:03:59,560 --> 00:04:01,490
a diagnosis can be reached.

90
00:04:01,490 --> 00:04:06,410
And could you come up with any other advantages?

91
00:04:06,410 --> 00:04:08,200
Let's look at another example where

92
00:04:08,200 --> 00:04:10,780
it's common to use Bayesian statistics

93
00:04:10,780 --> 00:04:13,150
and which we'll use as an inspiration for many

94
00:04:13,150 --> 00:04:15,250
of our examples in this course.

95
00:04:15,250 --> 00:04:17,630
And that is automotive examples.

96
00:04:17,630 --> 00:04:20,600
In this case, we look at self-driving vehicles,

97
00:04:20,600 --> 00:04:23,170
which rely on their ability to position surrounding

98
00:04:23,170 --> 00:04:27,460
vehicles in order to safely navigate its surroundings

99
00:04:27,460 --> 00:04:29,830
and not cause accidents.

100
00:04:29,830 --> 00:04:32,740
So let's assume that we have a self-driving vehicle, which

101
00:04:32,740 --> 00:04:35,320
is equipped with a bunch of sensors, that observes

102
00:04:35,320 --> 00:04:37,090
the traffic situation in front.

103
00:04:37,090 --> 00:04:39,790
The quantity of interest is then the relative position

104
00:04:39,790 --> 00:04:43,460
and velocity of the other vehicles at the current time.

105
00:04:43,460 --> 00:04:46,360
So we want to know the relative position and velocity

106
00:04:46,360 --> 00:04:48,310
to all of these vehicles here.

107
00:04:48,310 --> 00:04:51,310
To do this, we'll make use of noisy observations

108
00:04:51,310 --> 00:04:53,890
from the sensors on the self-driving vehicle.

109
00:04:53,890 --> 00:04:57,940
This could be wheel speed sensors, or inertial sensors,

110
00:04:57,940 --> 00:05:00,370
such as accelerometers and gyros,

111
00:05:00,370 --> 00:05:03,034
on our own vehicles to measure how we are moving.

112
00:05:03,034 --> 00:05:04,450
But we also need observations that

113
00:05:04,450 --> 00:05:07,090
give us information about the position of the other vehicles.

114
00:05:07,090 --> 00:05:09,689
This could, for example, be radar detections,

115
00:05:09,689 --> 00:05:11,230
which gives us the distance and angle

116
00:05:11,230 --> 00:05:14,680
to objects, or Lidar point clouds, or camera images

117
00:05:14,680 --> 00:05:15,790
and so on.

118
00:05:15,790 --> 00:05:19,060
One aspect I would like to highlight with this application

119
00:05:19,060 --> 00:05:22,570
is that, in Bayesian statistics, vehicle motions

120
00:05:22,570 --> 00:05:24,340
are modeled stochastically.

121
00:05:24,340 --> 00:05:27,610
That is, we describe likely movements of the vehicles using

122
00:05:27,610 --> 00:05:30,820
stochastic models, which helps us rule out

123
00:05:30,820 --> 00:05:33,400
unrealistic trajectories.

124
00:05:33,400 --> 00:05:36,860
If we look at the scenario here in a Bayesian perspective,

125
00:05:36,860 --> 00:05:40,690
we would say that it's likely that this vehicle here

126
00:05:40,690 --> 00:05:42,190
moves something like this.

127
00:05:42,190 --> 00:05:47,700


128
00:05:47,700 --> 00:05:50,670
That is, it is following the road more or less.

129
00:05:50,670 --> 00:05:56,200
And it's unlikely that it will make a sharp U-turn like this.

130
00:05:56,200 --> 00:05:58,120
Now, one possible concern one could

131
00:05:58,120 --> 00:06:02,110
have is whether vehicles' motions actually are random,

132
00:06:02,110 --> 00:06:05,830
and this underlines the basic Bayesian assumptions.

133
00:06:05,830 --> 00:06:07,540
For completeness, we should mention

134
00:06:07,540 --> 00:06:10,690
that there are two main strategies to decision making,

135
00:06:10,690 --> 00:06:14,770
using Bayesian statistics, which we'll focus on in this course,

136
00:06:14,770 --> 00:06:17,290
and using frequentist statistics.

137
00:06:17,290 --> 00:06:19,420
The main difference between these two

138
00:06:19,420 --> 00:06:21,600
are mainly on a philosophical level,

139
00:06:21,600 --> 00:06:24,940
but which also has consequences on the actual calculations that

140
00:06:24,940 --> 00:06:27,910
are made and the models that are used.

141
00:06:27,910 --> 00:06:31,510
So in frequentist statistics, the quantities of interest

142
00:06:31,510 --> 00:06:35,800
are described as unknown and deterministic and not

143
00:06:35,800 --> 00:06:39,580
unknown and stochastic or random as in Bayesian statistics.

144
00:06:39,580 --> 00:06:41,560
As a consequence, in the medical example

145
00:06:41,560 --> 00:06:43,840
that we discussed earlier, a frequentist

146
00:06:43,840 --> 00:06:46,120
would not state that, given the observations,

147
00:06:46,120 --> 00:06:49,090
the patient has a flu with 97% probability

148
00:06:49,090 --> 00:06:51,580
and the common cold with the 3% probability.

149
00:06:51,580 --> 00:06:54,820
But will rather say something like, based on observations,

150
00:06:54,820 --> 00:06:57,470
the patient's most likely has the flu.

151
00:06:57,470 --> 00:06:59,560
And the differences is that, for a Bayesian,

152
00:06:59,560 --> 00:07:01,810
the answer is a probability distribution

153
00:07:01,810 --> 00:07:05,110
of the disease based on our observations,

154
00:07:05,110 --> 00:07:07,510
while for a frequentist, the answer is

155
00:07:07,510 --> 00:07:10,990
the most likely disease based on our observations.

156
00:07:10,990 --> 00:07:12,910
Let us look at another example and try

157
00:07:12,910 --> 00:07:14,950
to highlight the difference between viewing

158
00:07:14,950 --> 00:07:18,430
the quantity of interest as unknown but deterministic

159
00:07:18,430 --> 00:07:21,400
compared to unknown and random.

160
00:07:21,400 --> 00:07:23,020
Let's say that we wish to estimate

161
00:07:23,020 --> 00:07:24,820
the height of the Eiffel Tower.

162
00:07:24,820 --> 00:07:26,800
Is the height random or not?

163
00:07:26,800 --> 00:07:29,080
Well, in a frequentist perspective,

164
00:07:29,080 --> 00:07:33,370
the tower has a certain height and it's therefore not random.

165
00:07:33,370 --> 00:07:37,000
That is, it has some fixed but unknown value.

166
00:07:37,000 --> 00:07:41,350
Using the Bayesian perspective, we describe our uncertainties

167
00:07:41,350 --> 00:07:44,290
in the height by viewing it as a random variable

168
00:07:44,290 --> 00:07:46,540
with a certain probability distribution.

169
00:07:46,540 --> 00:07:49,690
In a sense, describing the height as random

170
00:07:49,690 --> 00:07:53,690
becomes a useful tool to incorporate prior information

171
00:07:53,690 --> 00:07:56,050
that we have about the height of the Eiffel Tower

172
00:07:56,050 --> 00:07:58,390
before we make any observations.

173
00:07:58,390 --> 00:08:01,210
I encourage you to think about this yourself.

174
00:08:01,210 --> 00:08:04,630
What do you think makes most sense and why?

175
00:08:04,630 --> 00:08:07,480
If you're not fully convinced that the Bayesian perspective

176
00:08:07,480 --> 00:08:10,510
makes sense, I hope that I will be able to convince you

177
00:08:10,510 --> 00:08:12,220
during this course.

178
00:08:12,220 --> 00:08:14,080
I should also mention that, in many cases,

179
00:08:14,080 --> 00:08:16,120
the frequentist and the Bayesian would end up

180
00:08:16,120 --> 00:08:19,150
with the same answer, but there are also many cases

181
00:08:19,150 --> 00:08:21,370
where they would disagree.

182
00:08:21,370 --> 00:08:24,160
We will now leave the frequentist perspective

183
00:08:24,160 --> 00:08:27,580
and focus our attention again on Bayesian statistics

184
00:08:27,580 --> 00:08:31,690
by giving you an overview of the Bayesian strategy.

185
00:08:31,690 --> 00:08:35,650
So suppose we wish to estimate some quantity of interest-- we

186
00:08:35,650 --> 00:08:39,030
call this theta, given measurements on theta

187
00:08:39,030 --> 00:08:41,260
that we call y.

188
00:08:41,260 --> 00:08:43,360
The key steps in Bayesian methods

189
00:08:43,360 --> 00:08:47,080
is that we first model everything that we need.

190
00:08:47,080 --> 00:08:49,220
In this case, we need two models.

191
00:08:49,220 --> 00:08:52,300
The first model describes what we know about theta

192
00:08:52,300 --> 00:08:54,890
before making any observations.

193
00:08:54,890 --> 00:08:57,460
And we do this using what's called a prior, which is

194
00:08:57,460 --> 00:09:00,340
this distribution p of theta.

195
00:09:00,340 --> 00:09:02,110
In the example of the Eiffel Tower,

196
00:09:02,110 --> 00:09:04,960
we would here say that we know that the height of the Eiffel

197
00:09:04,960 --> 00:09:09,290
Tower is somewhere between 250 and 350 meters.

198
00:09:09,290 --> 00:09:10,960
And we would describe this knowledge

199
00:09:10,960 --> 00:09:14,320
using our prior distribution like this.

200
00:09:14,320 --> 00:09:16,390
So this is some prior knowledge that we

201
00:09:16,390 --> 00:09:18,280
have about the height of the tower

202
00:09:18,280 --> 00:09:20,710
before we even make any measurements.

203
00:09:20,710 --> 00:09:22,870
The second model that we need in this case

204
00:09:22,870 --> 00:09:25,390
is a probabilistic model of how the measurements

205
00:09:25,390 --> 00:09:28,120
y relate to theta.

206
00:09:28,120 --> 00:09:30,700
And we do this using the probability

207
00:09:30,700 --> 00:09:36,260
density of y condition on that we know theta.

208
00:09:36,260 --> 00:09:38,690
Note that, as we are often interested in this

209
00:09:38,690 --> 00:09:41,750
as a function of theta, and that this is not a probability

210
00:09:41,750 --> 00:09:45,740
density over theta but over y, to make this clear

211
00:09:45,740 --> 00:09:48,770
we call this the likelihood of theta.

212
00:09:48,770 --> 00:09:51,650
The second step is to make a measurement update, where

213
00:09:51,650 --> 00:09:54,380
we combine what we knew from before from this

214
00:09:54,380 --> 00:09:57,500
prior with our measurements, which

215
00:09:57,500 --> 00:10:00,170
is described by this density, which we also

216
00:10:00,170 --> 00:10:02,690
called a likelihood, right?

217
00:10:02,690 --> 00:10:05,000
We summarize our knowledge about theta

218
00:10:05,000 --> 00:10:07,400
in what's called a posterior density, which

219
00:10:07,400 --> 00:10:12,110
is the probability density of theta after we observed y.

220
00:10:12,110 --> 00:10:14,030
So y is now known.

221
00:10:14,030 --> 00:10:16,550
Our observation is now known.

222
00:10:16,550 --> 00:10:20,180
And the final step is to make a decision regarding theta.

223
00:10:20,180 --> 00:10:21,800
In the example of the Eiffel Tower,

224
00:10:21,800 --> 00:10:24,770
we would like to make a point estimate of the height.

225
00:10:24,770 --> 00:10:27,500
That is, we would like to decide on one value of the height

226
00:10:27,500 --> 00:10:31,070
instead of having this distribution of the height.

227
00:10:31,070 --> 00:10:33,350
In general, we do Bayesian decision

228
00:10:33,350 --> 00:10:37,970
making by taking what we know about a parameter theta,

229
00:10:37,970 --> 00:10:40,460
as described by this posterior density,

230
00:10:40,460 --> 00:10:42,890
and combine that with a loss function

231
00:10:42,890 --> 00:10:45,860
that describes what are important aspects

232
00:10:45,860 --> 00:10:46,820
in our decision.

233
00:10:46,820 --> 00:10:50,240
The goal is then to compute an optimal decision that

234
00:10:50,240 --> 00:10:54,560
minimizes the average loss over possible values of theta,

235
00:10:54,560 --> 00:10:57,290
as described by our posterior density.

236
00:10:57,290 --> 00:10:59,330
In the Eiffel Tower example, we typically

237
00:10:59,330 --> 00:11:01,730
would like to find an estimate that is either

238
00:11:01,730 --> 00:11:06,290
the most probable height or the estimate which, on average,

239
00:11:06,290 --> 00:11:08,480
if we do this multiple times with new measurements

240
00:11:08,480 --> 00:11:11,090
every time, has the lowest error.

241
00:11:11,090 --> 00:11:16,800
We call this the MAP and the MMSE estimates, respectively.

242
00:11:16,800 --> 00:11:18,650
And we will look at the loss functions

243
00:11:18,650 --> 00:11:20,730
for these in a later lecture.

244
00:11:20,730 --> 00:11:22,280
So, in this course, we will mainly

245
00:11:22,280 --> 00:11:24,290
focus on the second step.

246
00:11:24,290 --> 00:11:27,590
But we'll also discuss how we derive our models

247
00:11:27,590 --> 00:11:30,750
and look at standard ways of decision making.

248
00:11:30,750 --> 00:11:33,200
We wrap things up with a self-assessment question

249
00:11:33,200 --> 00:11:35,440
for you to think about.

250
00:11:35,440 --> 00:11:40,915


