0
00:00:00,000 --> 00:00:00,500


1
00:00:00,500 --> 00:00:02,009
In this lecture, we're going to go

2
00:00:02,009 --> 00:00:05,020
through three of the most important basic tools

3
00:00:05,020 --> 00:00:07,830
that we're going to use throughout this course.

4
00:00:07,830 --> 00:00:10,650
And that is expectation, covariance,

5
00:00:10,650 --> 00:00:12,840
and the Gaussian distribution.

6
00:00:12,840 --> 00:00:14,895
That actually sounds like the title of a movie

7
00:00:14,895 --> 00:00:16,560
that I would watch.

8
00:00:16,560 --> 00:00:19,800
Now, anyway, the single most important distribution

9
00:00:19,800 --> 00:00:22,080
for sensor fusion and nonlinear filtering

10
00:00:22,080 --> 00:00:24,150
is the Gaussian distribution.

11
00:00:24,150 --> 00:00:27,480
In many cases, our goal is to describe our results

12
00:00:27,480 --> 00:00:30,330
using the mean vector or expected value

13
00:00:30,330 --> 00:00:33,910
and the covariance matrix of a Gaussian distribution.

14
00:00:33,910 --> 00:00:36,050
And if our result is not Gaussian,

15
00:00:36,050 --> 00:00:39,690
we anyway tend to approximate our results using the mean

16
00:00:39,690 --> 00:00:43,140
and covariance of this non-Gaussian distribution,

17
00:00:43,140 --> 00:00:46,200
even though these do not fully capture all the facets

18
00:00:46,200 --> 00:00:48,330
of our actual distribution.

19
00:00:48,330 --> 00:00:50,910
So as we said, probability distributions

20
00:00:50,910 --> 00:00:54,000
are also characterized by their mean vectors

21
00:00:54,000 --> 00:00:56,490
and covariance matrices.

22
00:00:56,490 --> 00:00:59,350
So how do we calculate this?

23
00:00:59,350 --> 00:01:03,210
Well, the expected value or mean of an m-dimensional random

24
00:01:03,210 --> 00:01:08,640
vector x, defined like this, is the expected value of x.

25
00:01:08,640 --> 00:01:10,590
Now, the expectation of x is defined

26
00:01:10,590 --> 00:01:14,850
as an integral of the vector x weighted by the probability

27
00:01:14,850 --> 00:01:16,620
density of that vector.

28
00:01:16,620 --> 00:01:19,860
So what we have here is that for values

29
00:01:19,860 --> 00:01:23,040
of x which have high probability density

30
00:01:23,040 --> 00:01:24,840
will influence our expected value

31
00:01:24,840 --> 00:01:29,760
more than values of x which have low probability density.

32
00:01:29,760 --> 00:01:36,240
Often we denote this mean vector here as either mu, or x bar

33
00:01:36,240 --> 00:01:39,100
in this case.

34
00:01:39,100 --> 00:01:43,160
And we call this the first moment of p of x.

35
00:01:43,160 --> 00:01:46,540
Now, the notation here is simply shorthand that we

36
00:01:46,540 --> 00:01:49,210
integrated from minus infinity to infinity

37
00:01:49,210 --> 00:01:51,370
for all the dimensions of x.

38
00:01:51,370 --> 00:01:53,260
So that was the expected value.

39
00:01:53,260 --> 00:01:57,070
Now, the covariance of x or the second central moment

40
00:01:57,070 --> 00:02:00,820
of p of x is written like this and is defined

41
00:02:00,820 --> 00:02:06,340
as the expectation of x minus its mean times the same thing

42
00:02:06,340 --> 00:02:07,510
transposed.

43
00:02:07,510 --> 00:02:09,220
Now, we can view this factor here

44
00:02:09,220 --> 00:02:11,860
as the distance between x and its mean,

45
00:02:11,860 --> 00:02:16,560
that is, how much does x spread around its mean value?

46
00:02:16,560 --> 00:02:18,780
Now, if we write it like this, we

47
00:02:18,780 --> 00:02:20,910
assume that x is a column vector,

48
00:02:20,910 --> 00:02:23,220
as we will do throughout the whole of this course,

49
00:02:23,220 --> 00:02:26,340
as this here, the product here, should be an outer product.

50
00:02:26,340 --> 00:02:33,010
That is, we have a tall matrix times a wide matrix.

51
00:02:33,010 --> 00:02:40,970
So this product here is an m by m squared matrix, which

52
00:02:40,970 --> 00:02:44,030
is symmetric and positive semidefinite, as this

53
00:02:44,030 --> 00:02:46,100
is a square here.

54
00:02:46,100 --> 00:02:49,340
So it needs to be positive semidefinite.

55
00:02:49,340 --> 00:02:51,950
So that is the definition of the covariance matrix

56
00:02:51,950 --> 00:02:53,960
of a random variable x.

57
00:02:53,960 --> 00:02:56,570
Now, for discrete-valued random variables,

58
00:02:56,570 --> 00:02:58,490
the above integrals are then replaced

59
00:02:58,490 --> 00:03:00,230
by corresponding summations.

60
00:03:00,230 --> 00:03:03,260
So we have summation here instead.

61
00:03:03,260 --> 00:03:05,510
So let us look a bit more into how

62
00:03:05,510 --> 00:03:09,680
we can interpret this covariance matrix using an example where

63
00:03:09,680 --> 00:03:13,880
we have independent samples from a zero-mean two-dimensional

64
00:03:13,880 --> 00:03:17,840
random vector x, which has the components x1 and x2.

65
00:03:17,840 --> 00:03:21,800
Now, our samples are plotted in red here in this figure.

66
00:03:21,800 --> 00:03:24,800
The question is if you can guess the covariance matrix

67
00:03:24,800 --> 00:03:27,560
of x using these samples here.

68
00:03:27,560 --> 00:03:29,570
Now, to help you out a bit, we have also

69
00:03:29,570 --> 00:03:31,370
plotted to one standard deviation

70
00:03:31,370 --> 00:03:33,770
and three standard deviation contours

71
00:03:33,770 --> 00:03:34,970
of the distribution of x.

72
00:03:34,970 --> 00:03:38,900
So we have three sigma contour here,

73
00:03:38,900 --> 00:03:43,480
and we have one sigma contour here.

74
00:03:43,480 --> 00:03:45,630
Additionally, we can mention that the covariance

75
00:03:45,630 --> 00:03:47,790
of a two-dimensional random variable

76
00:03:47,790 --> 00:03:49,590
has the following structure.

77
00:03:49,590 --> 00:04:16,310
So the covariance of x can be written like this, where

78
00:04:16,310 --> 00:04:21,870
sigma 1 is the standard deviation of x1 and sigma 2

79
00:04:21,870 --> 00:04:25,860
is the standard deviation of x2.

80
00:04:25,860 --> 00:04:28,080
And the factor rho here is what's

81
00:04:28,080 --> 00:04:30,120
called a correlation factor.

82
00:04:30,120 --> 00:04:34,020
So what do you think the covariance of x is?

83
00:04:34,020 --> 00:04:36,900
I would encourage you to pause the video here and think

84
00:04:36,900 --> 00:04:39,180
about this yourself for a couple of minutes.

85
00:04:39,180 --> 00:04:41,190
And then we can go through the results together.

86
00:04:41,190 --> 00:04:45,610


87
00:04:45,610 --> 00:04:48,550
So as I mentioned, the covariance matrix

88
00:04:48,550 --> 00:04:52,380
is a measure for how the samples of a random vector

89
00:04:52,380 --> 00:04:55,120
have spread around its mean.

90
00:04:55,120 --> 00:04:58,340
Now, as x is a zero-mean random vector,

91
00:04:58,340 --> 00:05:06,810
its mean is located here at 0, 0.

92
00:05:06,810 --> 00:05:09,350


93
00:05:09,350 --> 00:05:12,120
And if we look at the structure of the covariance matrix

94
00:05:12,120 --> 00:05:14,380
that we'll find here, we see that we basically

95
00:05:14,380 --> 00:05:19,090
need to find the standard deviation of x1 and x2

96
00:05:19,090 --> 00:05:21,940
plus the correlation factor rho.

97
00:05:21,940 --> 00:05:25,780
Now, the standard deviation of x1 and x2

98
00:05:25,780 --> 00:05:28,240
can be found by projecting this one sigma

99
00:05:28,240 --> 00:05:32,290
contour onto both dimensions here and then

100
00:05:32,290 --> 00:05:35,890
looking at the distance between the projected

101
00:05:35,890 --> 00:05:38,700
contour and the mean.

102
00:05:38,700 --> 00:05:58,482
So if we do this, we get sigma 1 and sigma 2

103
00:05:58,482 --> 00:06:08,570
approximately equal to 1.4.

104
00:06:08,570 --> 00:06:11,410
So we can start filling in our covariance matrix here.

105
00:06:11,410 --> 00:06:29,890


106
00:06:29,890 --> 00:06:32,560
So what about the correlation factor?

107
00:06:32,560 --> 00:06:35,050
So we know that the correlation factor

108
00:06:35,050 --> 00:06:39,250
is a value between minus 1 and 1 that tells us how correlated

109
00:06:39,250 --> 00:06:40,960
two random variables are.

110
00:06:40,960 --> 00:06:43,810
If the correlation factor is, for example, 1,

111
00:06:43,810 --> 00:06:47,420
the two random variables are fully positively correlated,

112
00:06:47,420 --> 00:06:49,690
meaning that x1 here, for example,

113
00:06:49,690 --> 00:06:54,050
is a deterministic positive function of x2, and vice versa.

114
00:06:54,050 --> 00:06:55,600
Now, if we look at our samples, we

115
00:06:55,600 --> 00:06:58,450
see that there is quite a significant positive

116
00:06:58,450 --> 00:06:59,320
correlation here.

117
00:06:59,320 --> 00:07:01,810
So if we have a high value on x1,

118
00:07:01,810 --> 00:07:05,080
there's a high probability that x2 also has a high value.

119
00:07:05,080 --> 00:07:07,660
But it's definitely not a deterministic mapping, right,

120
00:07:07,660 --> 00:07:11,890
as if it were, all the samples here would collapse and fall

121
00:07:11,890 --> 00:07:14,880
on the line like this.

122
00:07:14,880 --> 00:07:16,470
But that's not the case, right?

123
00:07:16,470 --> 00:07:19,050
So we have some spread around us.

124
00:07:19,050 --> 00:07:24,020
So this would mean rho equal to 1,

125
00:07:24,020 --> 00:07:26,707
if all the samples were on this line.

126
00:07:26,707 --> 00:07:28,790
Now in this case, I would say that the correlation

127
00:07:28,790 --> 00:07:30,800
is somewhere around 0.9.

128
00:07:30,800 --> 00:07:32,600
So we have 0.9 here.

129
00:07:32,600 --> 00:07:38,630


130
00:07:38,630 --> 00:07:40,520
Putting this together, we get a final guess

131
00:07:40,520 --> 00:07:42,995
for our covariance matrix, which is--

132
00:07:42,995 --> 00:07:45,890
so the variance in each dimension is around 2.

133
00:07:45,890 --> 00:07:53,000
And we have a cross-covariance of 1.8.

134
00:07:53,000 --> 00:07:55,040
So does this match what you guessed?

135
00:07:55,040 --> 00:07:57,980


136
00:07:57,980 --> 00:08:01,370
So if we look at the covariance matrix for a random variable,

137
00:08:01,370 --> 00:08:04,310
it will tell us both the variance

138
00:08:04,310 --> 00:08:06,380
in the different dimensions, but it will also

139
00:08:06,380 --> 00:08:08,870
tell us how correlated the different dimensions are.

140
00:08:08,870 --> 00:08:13,916
So we have a cross-covariance of 1.8 here between x1 and x2.

141
00:08:13,916 --> 00:08:15,290
And this information is something

142
00:08:15,290 --> 00:08:18,350
that is used extensively in sensor fusion and nonlinear

143
00:08:18,350 --> 00:08:21,420
filtering, which we will see later in this course.

144
00:08:21,420 --> 00:08:23,240
Sometimes we're interested in finding

145
00:08:23,240 --> 00:08:26,630
the expected value of a random variable numerically.

146
00:08:26,630 --> 00:08:29,390
Perhaps we are not able to solve the involved integrals

147
00:08:29,390 --> 00:08:32,659
explicitly, or more commonly, when we do not actually

148
00:08:32,659 --> 00:08:35,600
know the underlying distribution but have access

149
00:08:35,600 --> 00:08:37,740
to a large number of samples from it.

150
00:08:37,740 --> 00:08:40,940
So in this case, we can use the law of large numbers, which

151
00:08:40,940 --> 00:08:43,880
states that sample averages converge

152
00:08:43,880 --> 00:08:47,330
to expected values for large sample sizes.

153
00:08:47,330 --> 00:08:51,890
Now, if x1, x2, and so on are independent and identically

154
00:08:51,890 --> 00:08:54,560
distributed random variables, distributed according

155
00:08:54,560 --> 00:08:58,520
to some distribution p of x, then the sample average,

156
00:08:58,520 --> 00:09:01,670
that is, we sum all the samples and divide

157
00:09:01,670 --> 00:09:04,040
by the number of samples, will converge

158
00:09:04,040 --> 00:09:06,270
to the actual expected value of x

159
00:09:06,270 --> 00:09:09,560
under p of x if we let the number of samples

160
00:09:09,560 --> 00:09:11,100
grow to infinity.

161
00:09:11,100 --> 00:09:14,150
Now, we can think of this as throwing a dice many times,

162
00:09:14,150 --> 00:09:16,160
where each roll of the dice would generate

163
00:09:16,160 --> 00:09:18,710
a new, independent, and identically distributed

164
00:09:18,710 --> 00:09:22,460
sample from a distribution of dice face values.

165
00:09:22,460 --> 00:09:24,800
If we do this sufficiently many times,

166
00:09:24,800 --> 00:09:27,080
eventually the average face value

167
00:09:27,080 --> 00:09:29,840
converges to expected value, which we can

168
00:09:29,840 --> 00:09:32,810
calculate to be 3.5, like this.

169
00:09:32,810 --> 00:09:36,870
So this is a way to numerically calculate expected values.

170
00:09:36,870 --> 00:09:39,980
So now that we know what the expected value and covariance

171
00:09:39,980 --> 00:09:44,450
is, it's time to look at the most important distribution

172
00:09:44,450 --> 00:09:46,820
of them all, at least in this course.

173
00:09:46,820 --> 00:09:49,250
And that is the Gaussian distribution.

174
00:09:49,250 --> 00:09:51,830
And as you will see, there is a clear connection

175
00:09:51,830 --> 00:09:54,740
with the mean and covariance of a random variable.

176
00:09:54,740 --> 00:09:57,710
So we write like this to denote that x

177
00:09:57,710 --> 00:10:02,600
is a Gaussian random variable with mean mu and covariance Q.

178
00:10:02,600 --> 00:10:05,690
And the pdf of x is then denoted like this, so p

179
00:10:05,690 --> 00:10:09,940
of x where we use this notation here to denote the Gaussian

180
00:10:09,940 --> 00:10:16,430
pdf as a function of x with the parameters mu and Q.

181
00:10:16,430 --> 00:10:19,040
Note that there's a important difference with what

182
00:10:19,040 --> 00:10:22,880
we write here, where we are saying that x is distributed

183
00:10:22,880 --> 00:10:24,470
according to Gaussian distribution

184
00:10:24,470 --> 00:10:27,680
with these parameters, mu and Q. But here we are referring

185
00:10:27,680 --> 00:10:31,400
to the actual function as a function of x

186
00:10:31,400 --> 00:10:33,050
with these parameters.

187
00:10:33,050 --> 00:10:35,480
So by definition, the Gaussian pdf

188
00:10:35,480 --> 00:10:37,810
can be expressed like this, where

189
00:10:37,810 --> 00:10:39,890
we have here a constant term, which

190
00:10:39,890 --> 00:10:43,400
is dependent on the covariance, times the exponential

191
00:10:43,400 --> 00:10:45,650
to the power of a quadratic function of x.

192
00:10:45,650 --> 00:10:49,160
We will have x minus its mean squared, which

193
00:10:49,160 --> 00:10:53,270
is then normalized by or scaled by the covariance matrix

194
00:10:53,270 --> 00:10:55,090
inverse.

195
00:10:55,090 --> 00:10:58,420
If we plot this function here in the scalar case for a Gaussian

196
00:10:58,420 --> 00:11:02,110
distribution with mean 0 and variance 1,

197
00:11:02,110 --> 00:11:05,500
we get this classical bell-shaped curve,

198
00:11:05,500 --> 00:11:09,280
where the peak is at the mean, which is 0 in this case,

199
00:11:09,280 --> 00:11:12,535
and the spread around the mean is dependent on the variance.

200
00:11:12,535 --> 00:11:18,110


201
00:11:18,110 --> 00:11:22,160
So note that a Gaussian random variable is completely

202
00:11:22,160 --> 00:11:25,740
determined by its mean and its covariance matrix.

203
00:11:25,740 --> 00:11:28,370
So we do not need any other parameters to describe

204
00:11:28,370 --> 00:11:30,050
a Gaussian distribution.

205
00:11:30,050 --> 00:11:33,350
Later in this course, we will use this extensively

206
00:11:33,350 --> 00:11:35,265
when we describe the results of our filters.

207
00:11:35,265 --> 00:11:37,780


208
00:11:37,780 --> 00:11:41,020
One of the nicer properties with Gaussian random variables

209
00:11:41,020 --> 00:11:43,480
is what happens when we have a linear combination

210
00:11:43,480 --> 00:11:46,040
of independent Gaussian random variables.

211
00:11:46,040 --> 00:11:49,060
So let x and y be Gaussian random variables

212
00:11:49,060 --> 00:11:51,490
with these means and covariances.

213
00:11:51,490 --> 00:11:54,160
If we now construct a new random variable

214
00:11:54,160 --> 00:11:57,610
z, which is then a linear combination of x and y

215
00:11:57,610 --> 00:12:00,610
like this, so z is equal to A times

216
00:12:00,610 --> 00:12:06,760
x plus B times y, where A and B are now deterministic matrices.

217
00:12:06,760 --> 00:12:11,050
Then z will also be a Gaussian random variable, which mean

218
00:12:11,050 --> 00:12:12,470
can be calculated like this.

219
00:12:12,470 --> 00:12:16,750
So the mean of z is the expected value of Ax plus By.

220
00:12:16,750 --> 00:12:20,200
Now, as we can always split the expectation of a sum

221
00:12:20,200 --> 00:12:24,940
into its individual components, we get the expected value of Ax

222
00:12:24,940 --> 00:12:27,720
plus the expected value of By.

223
00:12:27,720 --> 00:12:30,430
And A and B are deterministic matrices,

224
00:12:30,430 --> 00:12:32,770
we can move them outside of the expectations.

225
00:12:32,770 --> 00:12:35,020
So we get A times the expected value

226
00:12:35,020 --> 00:12:40,060
of x, which is mu x, plus B times the expected value of y,

227
00:12:40,060 --> 00:12:41,470
which is mu y.

228
00:12:41,470 --> 00:12:43,810
Now for the covariance, it's a bit more tricky.

229
00:12:43,810 --> 00:12:47,140
But by definition, the covariance of z

230
00:12:47,140 --> 00:12:50,230
is equal to the covariance of Ax plus By.

231
00:12:50,230 --> 00:12:53,350
We just insert what z is, right?

232
00:12:53,350 --> 00:12:55,890
Now, as x and y are independent, we

233
00:12:55,890 --> 00:12:59,020
can divide this covariance of this sum

234
00:12:59,020 --> 00:13:02,050
into the covariance of the different parts here

235
00:13:02,050 --> 00:13:03,760
or the different terms in the sum.

236
00:13:03,760 --> 00:13:06,170
Now, note that we cannot do this in general,

237
00:13:06,170 --> 00:13:09,010
as we need to consider the cross-covariance terms,

238
00:13:09,010 --> 00:13:16,160
which in this case is the cross-covariance of Ax,

239
00:13:16,160 --> 00:13:22,260
By and the cross-covariance of By and Ax.

240
00:13:22,260 --> 00:13:30,270


241
00:13:30,270 --> 00:13:34,605
But as x and y are independent, these terms here are 0.

242
00:13:34,605 --> 00:13:41,930


243
00:13:41,930 --> 00:13:43,830
So we can ignore them here.

244
00:13:43,830 --> 00:13:46,880
Now again, as A and B are deterministic,

245
00:13:46,880 --> 00:13:49,370
we can move them outside of the covariance.

246
00:13:49,370 --> 00:13:52,460
But instead of getting them just as a factor

247
00:13:52,460 --> 00:13:55,450
that we got here when we calculated the mean,

248
00:13:55,450 --> 00:13:57,440
we get them squared here.

249
00:13:57,440 --> 00:13:59,780
And as we're dealing with matrices,

250
00:13:59,780 --> 00:14:02,180
we need to be a bit careful about which order

251
00:14:02,180 --> 00:14:03,800
we do the square.

252
00:14:03,800 --> 00:14:08,120
So in this case, we get A times Qx, A transpose,

253
00:14:08,120 --> 00:14:11,140
plus BQy, B transpose.

254
00:14:11,140 --> 00:14:13,440
And this is a general rule for the covariance.

255
00:14:13,440 --> 00:14:17,240
So if you have a covariance of deterministic matrices

256
00:14:17,240 --> 00:14:19,520
times a random vector, the result

257
00:14:19,520 --> 00:14:25,690
will be A times the covariance of the random vector

258
00:14:25,690 --> 00:14:27,550
times A transpose.

259
00:14:27,550 --> 00:14:30,766
Now, if you feel that I went through this a bit quickly,

260
00:14:30,766 --> 00:14:32,140
I would highly recommend that you

261
00:14:32,140 --> 00:14:35,590
try to derive this expression here yourself

262
00:14:35,590 --> 00:14:38,680
using the definition of covariance

263
00:14:38,680 --> 00:14:41,230
that we presented at the beginning of this lecture

264
00:14:41,230 --> 00:14:43,630
and perhaps also brush up on rules related

265
00:14:43,630 --> 00:14:45,380
to the expectation operator.

266
00:14:45,380 --> 00:14:48,370
And I will think that you will find this well worth the effort

267
00:14:48,370 --> 00:14:52,270
when we start to discuss the actual material of this course.

268
00:14:52,270 --> 00:14:55,510
For your convenience, here is a summary of the basic statistics

269
00:14:55,510 --> 00:14:58,480
result that we have presented in this set of primer

270
00:14:58,480 --> 00:14:59,860
in statistics lectures.

271
00:14:59,860 --> 00:15:01,990
I would encourage you to familiarize yourself

272
00:15:01,990 --> 00:15:04,630
with these expressions and make sure that you

273
00:15:04,630 --> 00:15:06,700
understand what they are.

274
00:15:06,700 --> 00:15:09,010
You might also find this slide handy

275
00:15:09,010 --> 00:15:11,200
when we go through the results about nonlinear

276
00:15:11,200 --> 00:15:15,240
filtering and the rest of the lectures in this course.

277
00:15:15,240 --> 00:15:22,156


